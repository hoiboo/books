<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  
    <title>智能机器时代的伦理问题</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link rel="stylesheet" type="text/css" href="../stylesheet.css"/>
<link rel="stylesheet" type="text/css" href="../page_styles.css"/>

  

  <link href="../calibreHtmlOutBasicCss.css" type="text/css" rel="stylesheet" />
</head>
<body>


<div class="calibreMain">
  <div class="calibreEbookContent">
    
<h2 class="secondtitle" id="21PMQ1-3db257c46d3840eda9e0d384d9404fa9">智能机器时代的伦理问题</h2>
<p class="content">技术在带来利益的同时，也带来伦理的困境。想象一下，一辆无人驾驶汽车正驶向一个十字路口，打算右转弯，车上的传感器严密监控着右边骑车的人。突然一个蹒跚学步的孩子从母亲手中挣脱出来冲过马路，电脑必须在一瞬间决定避让骑车的人还是孩子。在这种情况下，没有时间收集更多的数据，也没有时间精心计算如何将伤害降到最小，更没有时间考虑哪一个人的生命胜过另一个人的，比如孩子和骑车的人。计算机将怎么做？</p>
<p class="content">这是经典思想实验“电车难题”的修正版本。在这个问题中，一辆飞驰而来的有轨电车即将撞死五个人。如果你能改变电车轨道，让它只撞死一个人，你会这么做吗？“电车难题”揭示了一个无法通过简单的道德或伦理来解决的难题。在电影《苏菲的抉择》中，梅丽尔·斯特里普饰演一位有两个孩子的波兰母亲，她在德国占领波兰期间参加抵抗运动，被捕后被送往奥斯威辛集中营。在那里，一名纳粹军官把她置于一个荒诞的处境，她必须在她的两个孩子中做出选择，哪个进毒气室，哪个进劳改营。在一时冲动之下，苏菲不得不做出一个可怕的选择，上面例子中的司机也是如此。这种道德困境有助于解释为什么无人机飞行员比传统飞行员患创伤后应激障碍的比例高。无人机飞行员在数千英里外的控制中心做出生死抉择，而真正的飞行员实际上要拿生命冒险。在《纽约时报》的一篇关于无人机操作员阿伦的文章中，埃亚勒·普雷斯（Eyal Press）写道：“在阿伦眼前展开的是一幅令人震撼的熟悉画面：在无人机袭击后，棺材被抬过街道。”尽管阿伦是一名经验丰富的军用无人机操作员，但他开始感到不适且情绪低落。他出现了虚弱的症状，包括恶心、皮肤红肿和慢性消化问题。“我感到非常不舒服。”他告诉记者。他对自己在日常任务执行中做出的关于杀谁、饶过谁的决定感到眩晕。</p>
<p class="content">2016—2017年，由麻省理工学院召集的一个国际研究团队开展过一个名为“道德机器实验”的项目，以评估不同文化背景的人如何处理这类困境。通过一个在线平台，他们收集了来自200多个国家和地区的200多万人的近4000万个关于开车的抉择。研究人员向受访者展示13种死亡不可避免的情境，有些决定可以说比其他的更容易做出。例如，司机应该保全宠物还是保全人？应该是大多数人还是少数人的生命享有优先权？但也有一些情境在伦理和道德方面非常棘手。例如，应该被保全的是身体健康的人还是残疾人？罪犯还是守法公民？在这个实验中，人们表现出一种明显的倾向，那就是人的生命优先于动物，多数人的生命优先于少数人，年轻人优先于年长者。因此，研究人员推断，这三种偏好可能被认为是机器伦理的基本组成部分。</p>
<p class="content">正如预期的那样，不同人的伦理偏好存在一些差异。男性和女性都更倾向于保全女性，但相对来说，女性表现出的倾向更强烈。有宗教信仰的人更倾向于保全人类，而非动物。研究还揭示了一些不同国家之间的明显差异：“在东部地区的国家（亚洲的以儒家文化为主流文化的国家和一些伊斯兰国家），保全年轻人而非年长者的倾向偏弱；在南部地区的国家（拉丁美洲和法语区的非洲国家），这个倾向更加明显。对于优先保全地位较高的人，这两个地区的倾向也是如此。”在南部这一组人中，“人们对人的偏爱远远弱于对宠物的偏爱”。有趣的是，“似乎在所有集群都是如此，通常人们（微弱）倾向于优先保护行人而非乘客，（适度）倾向于保护合法的人而不是非法的人”。在个人主义文化更浓厚的国家，人们更有可能保全年轻人；而在较贫穷的国家，人们对乱穿马路的人比遵守规则的行人更宽容。令人不安的是，经济不平等程度越高的国家，其受访者越愿意保全社会地位高的人。</p>
<p class="content">这项研究的作者之一伊亚德·拉万（Iyad Rahwan）说，这项研究的一个令人不安的暗示是，“那些思考机器伦理的人让它听起来像是你可以为机器人提出一套完美的规则”。“而我们用数据表明，并不存在通用的规则。”另一位合著者埃德蒙·阿瓦德（Edmond Awad）指出：“越来越多的人意识到，人工智能可能会对不同群体产生不同的道德后果。事实上，我们看到人们正致力于此——我认为这是有希望的。”正如奥迪德国自动驾驶汽车部门经理芭芭拉·魏格（Barbara Wege）所说：“我们需要就愿意承担哪些风险达成社会共识。”</p>
<p class="content">人工智能崛起带来的不仅仅是“电车难题”中的道德困境。正如索纳塔软件公司（Sonata Software）首席执行官斯里卡尔·雷迪（Srikar Reddy）以及我最近在世界经济论坛的一篇博客中所指出的，我们必须区分道义论伦理标准和目的论伦理标准，前者关注的是意图和方式，后者关注的是目的和结果。哪种方法更好，取决于技术和环境。“在自动驾驶汽车的例子中，追求一个既高效又对环境友好的无差错运输系统的目标，可能足以证明大规模收集不同条件下驾驶的数据以及基于人工智能应用的实验是合理的。”相比之下，鉴于在毫无戒心的人体上进行医学实验的可怕历史，基于大数据的医学试验很难在目的论的基础上站住脚。在这种情况下，基于意图和方式的道义论方法更有意义。</p>
<p class="content">自动化、人工智能和大数据带来的伦理和道德困境日渐显著。参与道德机器实验项目的人员总结说：“在人类历史上，我们从未允许过机器在没有实时监督的情况下瞬间自主决定谁该活、谁该死，而我们现在随时都会跨越这座桥。”如果你相信我的话，那会是在2030年。“在我们允许汽车做出道德决策之前，我们需要进行一次全球对话，向设计道德算法的公司和监管它们的决策制定者表达我们的偏好。”但问题是，伦理和道德的自动化并不能以算法的形式被自动化和展现出来。</p>



  </div>

  <div class="calibreEbNav">
    
      <a href="part0068.html" class="calibreAPrev">上一页
</a>
    

    <a href="../../vxq8y_jk.html" class="calibreAHome">目录
</a>

    
      <a href="part0070.html" class="calibreANext">下一页
</a>
    
  </div>

</div>

</body>
</html>
